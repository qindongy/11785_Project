{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_binary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ded5ybJyTw5r",
        "outputId": "d65f7250-a831-4574-ebc0-e0eabdd41263"
      },
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6pwRyUvTE4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de5d493-708a-4ec8-dcd8-8d5db6ccfa27"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36XgxaBCTT3Z"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import argparse\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pdb\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from transformers import BartTokenizer, BartForSequenceClassification\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUx1Fh_JzPl"
      },
      "source": [
        "#program logic\n",
        "# preprocess data: train data | validate data\n",
        "# intialized stage: predict --- > data split\n",
        "# iteration stage: fine_tune(data_split_output) ---> predict(fine_tune_output) ---> data_split(predict_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofsqY89-FAJt"
      },
      "source": [
        "# **preprocess data stage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUsPCS8dJXZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "a9f57856-4c92-4bd3-f941-b12be48afa4e"
      },
      "source": [
        "#preprocess data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#data=pd.read_csv('https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv')\n",
        "data=pd.read_csv('/content/gdrive/MyDrive/fox_news_data.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-940fb650e1ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#data=pd.read_csv('https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/fox_news_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minitial_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#only retrive sentence and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# train validate split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['tweet', 'class'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiRICTRqZ6PU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "pbKQRj8PgkGa",
        "outputId": "a876f1ac-2739-416d-f4f1-f24fd5f15427"
      },
      "source": [
        "data=pd.read_csv('/content/gdrive/MyDrive/fox_news_data.csv')\n",
        "data['Label']=pd.Series([1 if word==2 else word for word in data['Label']])\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Merkel would never say NO</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Expect more and more women to be asking .. \"wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Groping people in public wasn't already illega...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Merkel, possible the only person in charge who...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They know very well, no means NO! They need to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1523</th>\n",
              "      <td>No, 10000 Loose nuts off their meds!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1524</th>\n",
              "      <td>Just another ugly leftist.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1525</th>\n",
              "      <td>True. Most leftists ,esp female leftists have ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1526</th>\n",
              "      <td>First, lets get this straight: a white, gay ma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>White privilege ...work all your life to take ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1528 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text  Label\n",
              "0                             Merkel would never say NO      0\n",
              "1     Expect more and more women to be asking .. \"wh...      0\n",
              "2     Groping people in public wasn't already illega...      1\n",
              "3     Merkel, possible the only person in charge who...      0\n",
              "4     They know very well, no means NO! They need to...      0\n",
              "...                                                 ...    ...\n",
              "1523               No, 10000 Loose nuts off their meds!      0\n",
              "1524                         Just another ugly leftist.      0\n",
              "1525  True. Most leftists ,esp female leftists have ...      0\n",
              "1526  First, lets get this straight: a white, gay ma...      0\n",
              "1527  White privilege ...work all your life to take ...      0\n",
              "\n",
              "[1528 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wFQQRcxhD4F",
        "outputId": "c8ec7230-9406-4924-9b45-31bc9bf33890"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "data['Text'] = data.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "# Remove word that is not alphabetic\n",
        "data['Text'] = [[word for word in element if word.isalpha()] for element in data['Text']]\n",
        "\n",
        "# remove punctuation from each word\n",
        "#data['tweet'] = [[word.translate(table) for word in element] for element in data['tweet']]\n",
        "\n",
        "# Normalize word\n",
        "data['Text'] = [[word.lower() for word in element] for element in data['Text']]\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "#stop_words = stopwords.words('english')\n",
        "#data['tweet'] = [[word for word in element if not word in stop_words] for element in data['tweet']]\n",
        "\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "data['Text'] = data.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
        "#sentences=data['tweet']\n",
        "initial_data = data.copy() #only retrive sentence and label\n",
        "\n",
        "# train validate split\n",
        "train_data, valid_data = train_test_split(initial_data,test_size=0.3)\n",
        "train_data, valid_data = train_data.to_numpy(), valid_data.to_numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmGTuaihTWn7"
      },
      "source": [
        "#define model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\", num_labels = 2)\n",
        "#PATH=\"/content/gdrive/MyDrive/IDL_project/projectmodel_0408.pt\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
        "\n",
        "#load the model we trained\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model.load_state_dict(torch.load(PATH))\n",
        "model = model.to(device)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iNUK148FNU8"
      },
      "source": [
        "# **Initialization Stage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmVMEEjdUOGv",
        "outputId": "03033a23-f6bf-4d97-e94c-7febffdbe278"
      },
      "source": [
        "#prediction block\n",
        "#initialized first prediction\n",
        "sentences,labels = read_data(train_data)\n",
        "predict_data =predict(sentences, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished processing 0/1069 sentences\n",
            "Finished processing 1000/1069 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdmTiacUe7ko",
        "outputId": "4c9ac9cb-c40c-492f-e1cf-370965cada0f"
      },
      "source": [
        "predict_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to prison', 0, 1, 0.9756612181663513),\n",
              " ('first immigrate them then seduce them and then get them into acts where they dont want to hear no and then get them to jail if they are not submissive',\n",
              "  1,\n",
              "  0,\n",
              "  0.8794687986373901),\n",
              " ('i agree you know the other thing is this she is the leader of who either woof or agree with all of this garbage does that make her the founder of a hate group and all of her followers members of a hate group last time i checked if a white male posted these things and had such a following certainly this would be considered hate speech would it not',\n",
              "  0,\n",
              "  0,\n",
              "  0.8760384321212769),\n",
              " ('yeeeeup actually it a fence is it', 0, 1, 0.969562828540802),\n",
              " ('spoken like a true wookie', 0, 0, 0.6462669372558594)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWKFPAPK3yOJ",
        "outputId": "7ed6f483-0c73-4939-b55d-796154687ec3"
      },
      "source": [
        "#data split block\n",
        "#initialized stage: data_split\n",
        "fine_tuned_data,next_iter_data = data_split(predict_data,train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall Accuracy: 0.25350795135640786\n",
            "Check: True\n",
            "Total Dataset: 1069 Non-hate: 948 Hate: 121\n",
            "Nonhate Accuracy: 0.23734177215189872\n",
            "Hate Accuracy: 0.38016528925619836\n",
            "Number chosen: 47 6\n",
            "Top10 Confidence Non-hate Accuracy: 0.1702127659574468\n",
            "Top10 Confidence hate Accuracy: 0.0\n",
            "Check: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yckVr6HCFUfY"
      },
      "source": [
        "# **Iteration Stage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEjc7AsBD6_u",
        "outputId": "3d91482d-1a8f-4e04-f4ff-78c9900c40b1"
      },
      "source": [
        "#ITERATIVELY FINE TUNE MODEL\n",
        "\n",
        "for i in range(20):\n",
        "    print(\"\\n========fine tuning stage ============\\n\")\n",
        "    fine_tuning(fine_tuned_data, val_file= valid_data,tokenizer = tokenizer, model = model)\n",
        "\n",
        "    print(\"\\n========predict stage ============\\n\")\n",
        "    sentences,labels = read_data(next_iter_data)\n",
        "    predict_data =predict(sentences, labels)\n",
        "\n",
        "    print(\"\\n========data split stage ============\\n\")\n",
        "    fine_tuned_data,next_iter_data = data_split(predict_data,train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.8189713954925537\n",
            " Validation Accuracy: 0.5076\n",
            "Time: 5.795353651046753\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1035 sentences\n",
            "Finished processing 1000/1035 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.46570048309178746\n",
            "Check: True\n",
            "Total Dataset: 1035 Non-hate: 345 Hate: 690\n",
            "Nonhate Accuracy: 0.11014492753623188\n",
            "Hate Accuracy: 0.6434782608695652\n",
            "Number chosen: 17 34\n",
            "Top10 Confidence Non-hate Accuracy: 0.11764705882352941\n",
            "Top10 Confidence hate Accuracy: 0.23529411764705882\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:01\n",
            " Training Accuracy: 1.0000\n",
            "Time: 0.9199566841125488\n",
            " Validation Accuracy: 0.5098\n",
            "Time: 5.796773672103882\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1035 sentences\n",
            "Finished processing 1000/1035 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.4811594202898551\n",
            "Check: True\n",
            "Total Dataset: 1035 Non-hate: 339 Hate: 696\n",
            "Nonhate Accuracy: 0.11209439528023599\n",
            "Hate Accuracy: 0.6609195402298851\n",
            "Number chosen: 16 34\n",
            "Top10 Confidence Non-hate Accuracy: 0.125\n",
            "Top10 Confidence hate Accuracy: 0.47058823529411764\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 50})\n",
            "   50 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:01\n",
            " Training Accuracy: 1.0000\n",
            "Time: 0.9821929931640625\n",
            " Validation Accuracy: 0.5163\n",
            "Time: 5.797675371170044\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1035 sentences\n",
            "Finished processing 1000/1035 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.4995169082125604\n",
            "Check: True\n",
            "Total Dataset: 1035 Non-hate: 302 Hate: 733\n",
            "Nonhate Accuracy: 0.10927152317880795\n",
            "Hate Accuracy: 0.660300136425648\n",
            "Number chosen: 15 36\n",
            "Top10 Confidence Non-hate Accuracy: 0.13333333333333333\n",
            "Top10 Confidence hate Accuracy: 0.3611111111111111\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7228376865386963\n",
            " Validation Accuracy: 0.5272\n",
            "Time: 5.796563148498535\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1033 sentences\n",
            "Finished processing 1000/1033 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.515972894482091\n",
            "Check: True\n",
            "Total Dataset: 1033 Non-hate: 285 Hate: 748\n",
            "Nonhate Accuracy: 0.10877192982456141\n",
            "Hate Accuracy: 0.6711229946524064\n",
            "Number chosen: 14 37\n",
            "Top10 Confidence Non-hate Accuracy: 0.14285714285714285\n",
            "Top10 Confidence hate Accuracy: 0.5135135135135135\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:01\n",
            " Training Accuracy: 1.0000\n",
            "Time: 0.9902346134185791\n",
            " Validation Accuracy: 0.5251\n",
            "Time: 5.797451734542847\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1032 sentences\n",
            "Finished processing 1000/1032 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5087209302325582\n",
            "Check: True\n",
            "Total Dataset: 1032 Non-hate: 287 Hate: 745\n",
            "Nonhate Accuracy: 0.10801393728222997\n",
            "Hate Accuracy: 0.6630872483221476\n",
            "Number chosen: 14 37\n",
            "Top10 Confidence Non-hate Accuracy: 0.07142857142857142\n",
            "Top10 Confidence hate Accuracy: 0.2972972972972973\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:01\n",
            " Training Accuracy: 1.0000\n",
            "Time: 0.9244933128356934\n",
            " Validation Accuracy: 0.5207\n",
            "Time: 5.7964186668396\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1032 sentences\n",
            "Finished processing 1000/1032 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5155038759689923\n",
            "Check: True\n",
            "Total Dataset: 1032 Non-hate: 288 Hate: 744\n",
            "Nonhate Accuracy: 0.1076388888888889\n",
            "Hate Accuracy: 0.6733870967741935\n",
            "Number chosen: 14 37\n",
            "Top10 Confidence Non-hate Accuracy: 0.14285714285714285\n",
            "Top10 Confidence hate Accuracy: 0.40540540540540543\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7221601009368896\n",
            " Validation Accuracy: 0.5229\n",
            "Time: 5.79853081703186\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1032 sentences\n",
            "Finished processing 1000/1032 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5135658914728682\n",
            "Check: True\n",
            "Total Dataset: 1032 Non-hate: 286 Hate: 746\n",
            "Nonhate Accuracy: 0.10839160839160839\n",
            "Hate Accuracy: 0.6689008042895442\n",
            "Number chosen: 14 37\n",
            "Top10 Confidence Non-hate Accuracy: 0.07142857142857142\n",
            "Top10 Confidence hate Accuracy: 0.4594594594594595\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7870509624481201\n",
            " Validation Accuracy: 0.5316\n",
            "Time: 5.796818971633911\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1032 sentences\n",
            "Finished processing 1000/1032 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5203488372093024\n",
            "Check: True\n",
            "Total Dataset: 1032 Non-hate: 277 Hate: 755\n",
            "Nonhate Accuracy: 0.11191335740072202\n",
            "Hate Accuracy: 0.6701986754966888\n",
            "Number chosen: 13 37\n",
            "Top10 Confidence Non-hate Accuracy: 0.15384615384615385\n",
            "Top10 Confidence hate Accuracy: 0.43243243243243246\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 50})\n",
            "   50 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.6948766708374023\n",
            " Validation Accuracy: 0.5403\n",
            "Time: 5.796998739242554\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1032 sentences\n",
            "Finished processing 1000/1032 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5222868217054264\n",
            "Check: True\n",
            "Total Dataset: 1032 Non-hate: 270 Hate: 762\n",
            "Nonhate Accuracy: 0.1037037037037037\n",
            "Hate Accuracy: 0.6706036745406824\n",
            "Number chosen: 13 38\n",
            "Top10 Confidence Non-hate Accuracy: 0.07692307692307693\n",
            "Top10 Confidence hate Accuracy: 0.4473684210526316\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7922396659851074\n",
            " Validation Accuracy: 0.5490\n",
            "Time: 5.797183036804199\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1031 sentences\n",
            "Finished processing 1000/1031 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5295829291949563\n",
            "Check: True\n",
            "Total Dataset: 1031 Non-hate: 260 Hate: 771\n",
            "Nonhate Accuracy: 0.10384615384615385\n",
            "Hate Accuracy: 0.6731517509727627\n",
            "Number chosen: 13 38\n",
            "Top10 Confidence Non-hate Accuracy: 0.07692307692307693\n",
            "Top10 Confidence hate Accuracy: 0.4473684210526316\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.721268892288208\n",
            " Validation Accuracy: 0.5664\n",
            "Time: 5.796956777572632\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1031 sentences\n",
            "Finished processing 1000/1031 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5441319107662463\n",
            "Check: True\n",
            "Total Dataset: 1031 Non-hate: 243 Hate: 788\n",
            "Nonhate Accuracy: 0.10699588477366255\n",
            "Hate Accuracy: 0.6789340101522843\n",
            "Number chosen: 12 39\n",
            "Top10 Confidence Non-hate Accuracy: 0.08333333333333333\n",
            "Top10 Confidence hate Accuracy: 0.5641025641025641\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7874119281768799\n",
            " Validation Accuracy: 0.5686\n",
            "Time: 5.797133207321167\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1030 sentences\n",
            "Finished processing 1000/1030 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.541747572815534\n",
            "Check: True\n",
            "Total Dataset: 1030 Non-hate: 241 Hate: 789\n",
            "Nonhate Accuracy: 0.1078838174273859\n",
            "Hate Accuracy: 0.6742712294043093\n",
            "Number chosen: 12 39\n",
            "Top10 Confidence Non-hate Accuracy: 0.08333333333333333\n",
            "Top10 Confidence hate Accuracy: 0.48717948717948717\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.723832368850708\n",
            " Validation Accuracy: 0.5621\n",
            "Time: 5.7962939739227295\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1030 sentences\n",
            "Finished processing 1000/1030 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5398058252427185\n",
            "Check: True\n",
            "Total Dataset: 1030 Non-hate: 246 Hate: 784\n",
            "Nonhate Accuracy: 0.10569105691056911\n",
            "Hate Accuracy: 0.6760204081632653\n",
            "Number chosen: 12 39\n",
            "Top10 Confidence Non-hate Accuracy: 0.08333333333333333\n",
            "Top10 Confidence hate Accuracy: 0.48717948717948717\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7889840602874756\n",
            " Validation Accuracy: 0.5730\n",
            "Time: 5.797150135040283\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1030 sentences\n",
            "Finished processing 1000/1030 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5466019417475728\n",
            "Check: True\n",
            "Total Dataset: 1030 Non-hate: 239 Hate: 791\n",
            "Nonhate Accuracy: 0.1087866108786611\n",
            "Hate Accuracy: 0.6788874841972187\n",
            "Number chosen: 11 39\n",
            "Top10 Confidence Non-hate Accuracy: 0.09090909090909091\n",
            "Top10 Confidence hate Accuracy: 0.4358974358974359\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 50})\n",
            "   50 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.6906986236572266\n",
            " Validation Accuracy: 0.5730\n",
            "Time: 5.797912120819092\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1030 sentences\n",
            "Finished processing 1000/1030 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5514563106796116\n",
            "Check: True\n",
            "Total Dataset: 1030 Non-hate: 234 Hate: 796\n",
            "Nonhate Accuracy: 0.10683760683760683\n",
            "Hate Accuracy: 0.6821608040201005\n",
            "Number chosen: 11 39\n",
            "Top10 Confidence Non-hate Accuracy: 0.09090909090909091\n",
            "Top10 Confidence hate Accuracy: 0.48717948717948717\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 50})\n",
            "   50 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7605016231536865\n",
            " Validation Accuracy: 0.5752\n",
            "Time: 5.796210527420044\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1030 sentences\n",
            "Finished processing 1000/1030 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5504854368932038\n",
            "Check: True\n",
            "Total Dataset: 1030 Non-hate: 233 Hate: 797\n",
            "Nonhate Accuracy: 0.1072961373390558\n",
            "Hate Accuracy: 0.6800501882057717\n",
            "Number chosen: 11 39\n",
            "Top10 Confidence Non-hate Accuracy: 0.09090909090909091\n",
            "Top10 Confidence hate Accuracy: 0.5128205128205128\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 50})\n",
            "   50 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.6963427066802979\n",
            " Validation Accuracy: 0.5817\n",
            "Time: 5.7957000732421875\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1030 sentences\n",
            "Finished processing 1000/1030 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.558252427184466\n",
            "Check: True\n",
            "Total Dataset: 1030 Non-hate: 224 Hate: 806\n",
            "Nonhate Accuracy: 0.11160714285714286\n",
            "Hate Accuracy: 0.6823821339950372\n",
            "Number chosen: 11 40\n",
            "Top10 Confidence Non-hate Accuracy: 0.09090909090909091\n",
            "Top10 Confidence hate Accuracy: 0.575\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.6486454010009766\n",
            " Validation Accuracy: 0.5817\n",
            "Time: 5.79563307762146\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1029 sentences\n",
            "Finished processing 1000/1029 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5568513119533528\n",
            "Check: True\n",
            "Total Dataset: 1029 Non-hate: 221 Hate: 808\n",
            "Nonhate Accuracy: 0.1085972850678733\n",
            "Hate Accuracy: 0.6794554455445545\n",
            "Number chosen: 11 40\n",
            "Top10 Confidence Non-hate Accuracy: 0.09090909090909091\n",
            "Top10 Confidence hate Accuracy: 0.45\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:02\n",
            " Training Accuracy: 1.0000\n",
            "Time: 1.7865896224975586\n",
            " Validation Accuracy: 0.5817\n",
            "Time: 5.796985387802124\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1029 sentences\n",
            "Finished processing 1000/1029 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5617103984450923\n",
            "Check: True\n",
            "Total Dataset: 1029 Non-hate: 221 Hate: 808\n",
            "Nonhate Accuracy: 0.1085972850678733\n",
            "Hate Accuracy: 0.6856435643564357\n",
            "Number chosen: 11 40\n",
            "Top10 Confidence Non-hate Accuracy: 0.18181818181818182\n",
            "Top10 Confidence hate Accuracy: 0.5\n",
            "Check: True\n",
            "\n",
            "========fine tuning stage ============\n",
            "\n",
            "Train Label Distribution:\n",
            "Counter({1: 51})\n",
            "   51 training samples\n",
            "Train Loader: 4\n",
            "Validation Label Distribution:\n",
            "Counter({1: 324, 0: 135})\n",
            "  459 training samples\n",
            "Validation Loader: 4\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:01\n",
            " Training Accuracy: 1.0000\n",
            "Time: 0.7154319286346436\n",
            " Validation Accuracy: 0.5817\n",
            "Time: 5.807594537734985\n",
            "\n",
            "========predict stage ============\n",
            "\n",
            "Finished processing 0/1029 sentences\n",
            "Finished processing 1000/1029 sentences\n",
            "\n",
            "========data split stage ============\n",
            "\n",
            "Overall Accuracy: 0.5617103984450923\n",
            "Check: True\n",
            "Total Dataset: 1029 Non-hate: 219 Hate: 810\n",
            "Nonhate Accuracy: 0.1095890410958904\n",
            "Hate Accuracy: 0.6839506172839506\n",
            "Number chosen: 10 40\n",
            "Top10 Confidence Non-hate Accuracy: 0.1\n",
            "Top10 Confidence hate Accuracy: 0.475\n",
            "Check: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0n-cSDLFaV9"
      },
      "source": [
        "# **Helper function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F_YpO-8RImx"
      },
      "source": [
        "def read_data(data):\n",
        "    \n",
        "    sentences, labels = [], []\n",
        "\n",
        "    for item in data:\n",
        "        #item[0] : sentence\n",
        "        #item[1] : pred_label\n",
        "        sentences.append(item[0])\n",
        "        labels.append(item[1])\n",
        "\n",
        "    return sentences, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bv9l4Y2pCut"
      },
      "source": [
        "#read_data_test ---check predict.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZDQIhlDSKoH"
      },
      "source": [
        "def predict(sentences, labels):\n",
        "    preds, output_labels = [], []\n",
        "    pred_label_score = []\n",
        "    mapping_dict = {'NON_HATE':0,'HATE':1}\n",
        "    classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device = 0)\n",
        "\n",
        "    for i, (sentence, label) in enumerate(zip(sentences, labels)):\n",
        "        try:\n",
        "            pred = classifier(sentence)\n",
        "        except:\n",
        "            print('Faced error in predicting sentiment for: {}. Moving on...'.format(sentence))\n",
        "            continue\n",
        "\n",
        "        pred_label = pred[0]['label']\n",
        "\n",
        "        if pred_label in mapping_dict:\n",
        "            pred_label = mapping_dict[pred_label]\n",
        "\n",
        "        # pred_label = pred_label.lower()\n",
        "        # label = label.lower()\n",
        "        preds.append(pred_label)\n",
        "        output_labels.append(label)\n",
        "        pred_label_score.append((sentence, pred_label, label, pred[0]['score']))\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print('Finished processing {}/{} sentences'.format(i, len(sentences)))  \n",
        "              \n",
        "    return pred_label_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85hxA23913Js"
      },
      "source": [
        "def data_split(data, original_train_data):\n",
        "\n",
        "    n_nonhat, n_hat = count(data)\n",
        "\n",
        "    overall_accuracy = np.array([pred_label == label for (sentence, pred_label, label, score) in data])\n",
        "    print('Overall Accuracy:', np.sum(overall_accuracy)/len(data))\n",
        "\n",
        "    sorted_pred_nonhate, sorted_pred_hate = pred_result_split(data)\n",
        "\n",
        "\n",
        "    ##Total number of samples in top10% high confidence predictions for each class\n",
        "    # n_top10_neither = n_nei // 20   #min(args.positive, n_pos) #(n_pos * args.positive)//100\n",
        "    # n_top10_offensive = n_off // 20   #min(args.negative, n_neg)#(n_neg * args.negative)//100\n",
        "    n_top10_nonhate = n_nonhat // 20\n",
        "    n_top10_hate = n_hat // 20    #0#(n_neu * args.neutral)//100\n",
        "\n",
        "    print('Number chosen:', n_top10_nonhate, n_top10_hate)\n",
        "    \n",
        "    \n",
        "    #Find top 10 percent accuracy\n",
        "    # neither_top10_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(sorted_pred_neither) if i < n_top10_neither ])\n",
        "    # offensive_top10_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(sorted_pred_offensive) if i < n_top10_offensive ])\n",
        "    nonhate_top10_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(sorted_pred_nonhate) if i < n_top10_nonhate ])\n",
        "    hate_top10_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(sorted_pred_hate) if i < n_top10_hate ])\n",
        "\n",
        "    print('Top10 Confidence Non-hate Accuracy:', np.sum(nonhate_top10_accuracy) / n_top10_nonhate)\n",
        "    print('Top10 Confidence hate Accuracy:', np.sum(hate_top10_accuracy) / n_top10_hate)\n",
        "\n",
        "    \n",
        "    #Pick top chosen percent\n",
        "    # top_confidence_neither = sorted_pred_neither[:n_top10_neither]\n",
        "    # top_confidence_offensive = sorted_pred_offensive[:n_top10_offensive]\n",
        "    top_confidence_nonhate = sorted_pred_hate[:n_top10_nonhate]\n",
        "    top_confidence_hate = sorted_pred_hate[:n_top10_hate]\n",
        "\n",
        "    fine_tune_data = top_confidence_nonhate + top_confidence_hate\n",
        "    random.shuffle(fine_tune_data)\n",
        "    print('Check:', len(fine_tune_data) == len(top_confidence_nonhate) + len(top_confidence_hate))\n",
        "\n",
        "    #Collecting chosen sentences and removing them from original dataset, and saving\n",
        "    chosen_sentences = [sentence for (sentence, pred_label, label, score) in fine_tune_data]\n",
        "\n",
        "    next_iteration_data = []\n",
        "    for (tweet, label) in original_train_data:\n",
        "        if tweet not in chosen_sentences:\n",
        "            next_iteration_data.append((tweet, label))\n",
        "\n",
        "    return fine_tune_data, next_iteration_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkyKC-Ji1UvL"
      },
      "source": [
        "def count(data):\n",
        "    predicted_labels = [pred_label for (sentence, pred_label, label, score) in data]\n",
        "    n_non_hat = 0\n",
        "    n_hat = 0\n",
        "\n",
        "    for label in predicted_labels:\n",
        "        if label == 0:\n",
        "            n_non_hat += 1\n",
        "        elif label == 1:\n",
        "            n_hat += 1\n",
        "\n",
        "    return n_non_hat, n_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZApRVVA2h0q"
      },
      "source": [
        "#helper used in data_split function\n",
        "def pred_result_split(data):\n",
        "    \n",
        "    #Store in different dictionaries based on actual label\n",
        "    # pred_neither = []\n",
        "    # pred_offensive = []\n",
        "    pred_nonhate = []\n",
        "    pred_hate = []\n",
        "\n",
        "    for (sentence, pred_label, label, score) in data:\n",
        "        if pred_label == 0:\n",
        "            pred_nonhate.append((sentence, pred_label, label, score))\n",
        "        elif pred_label == 1:\n",
        "            pred_hate.append((sentence, pred_label, label, score))\n",
        "        \n",
        "    print('Check:', (len(pred_nonhate) + len(pred_hate)) == len(data))\n",
        "    print('Total Dataset:', len(data), 'Non-hate:', len(pred_nonhate), 'Hate:', len(pred_hate))\n",
        "    \n",
        "    #Original Accuracy for predictions made\n",
        "    # neither_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(pred_neither)])\n",
        "    # offensive_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(pred_offensive)])\n",
        "    nonhate_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(pred_nonhate)])\n",
        "    hate_accuracy = np.array([pred_label == label for i, (sentence, pred_label, label, score) in enumerate(pred_hate)])\n",
        "\n",
        "    # print('Neither Accuray:', np.sum(neither_accuracy)/len(pred_neither))\n",
        "    # print('Offensive Accuracy:', np.sum(offensive_accuracy)/len(pred_offensive))\n",
        "    print('Nonhate Accuracy:', np.sum(nonhate_accuracy)/len(pred_nonhate))\n",
        "    print('Hate Accuracy:', np.sum(hate_accuracy)/len(pred_hate))\n",
        "\n",
        "    ##Sorting by predicting confidence\n",
        "    sorted_pred_nonhate = sorted(pred_nonhate, key=lambda k: k[3], reverse=True)\n",
        "    sorted_pred_hate = sorted(pred_hate, key=lambda k: k[3], reverse=True)\n",
        "\n",
        "    return sorted_pred_nonhate, sorted_pred_hate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Jwh75l90ub"
      },
      "source": [
        "**Fine tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jelPmV719HsT"
      },
      "source": [
        "def fine_tuning(fine_tune_file, val_file,tokenizer, model):\n",
        "    #\n",
        "    #read tokenizer\n",
        "    #read model\n",
        "    sentences, labels = read_data(fine_tune_file) #label is pred_label from the prediction\n",
        "\n",
        "    label_distribution = Counter(labels)\n",
        "    print('Train Label Distribution:')\n",
        "    print(label_distribution)\n",
        "\n",
        "    batch_size = 16\n",
        "    num_epochs = 1\n",
        "    train_dataloader = create_loader(tokenizer, sentences, labels, batch_size) #create train loader\n",
        "    print('Train Loader:', len(train_dataloader))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "    total_steps = len(train_dataloader) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "    \n",
        "    #read validate data\n",
        "    sentences, labels = read_data(val_file)\n",
        "    label_distribution = Counter(labels)\n",
        "    print('Validation Label Distribution:')\n",
        "    print(label_distribution)\n",
        "\n",
        "    batch_size = 128\n",
        "    val_dataloader = create_loader(tokenizer, sentences, labels, batch_size)\n",
        "    print('Validation Loader:', len(val_dataloader))\n",
        "\n",
        "    #fine tuning loop\n",
        "    for i in range(num_epochs):\n",
        "        train_epoch(train_dataloader, model, optimizer, scheduler)\n",
        "        eval_epoch(val_dataloader, model)\n",
        "\n",
        "    #save model\n",
        "    #save tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwP2M1h9Vdj"
      },
      "source": [
        "def tokenize_text(tokeinzer, sentences, labels):\n",
        "\n",
        "    data_input_ids, data_attention_masks, output_labels = [], [], []\n",
        "\n",
        "    #TODO: can this be optimized?\n",
        "    max_num_tokens = 0\n",
        "    for sent, label in zip(sentences, labels):\n",
        "        tokenized_output = tokeinzer(sent)\n",
        "        sent_input_ids = torch.tensor(tokenized_output['input_ids'])\n",
        "        sent_attention_mask = torch.tensor(tokenized_output['attention_mask'])\n",
        "        if len(sent_input_ids) > max_num_tokens:\n",
        "            max_num_tokens = len(sent_input_ids)\n",
        "\n",
        "    for sent, label in zip(sentences, labels):\n",
        "        tokenized_output = tokeinzer(sent, max_length=max_num_tokens, pad_to_max_length = True)\n",
        "        sent_input_ids = torch.tensor(tokenized_output['input_ids'])\n",
        "        sent_attention_mask = torch.tensor(tokenized_output['attention_mask'])\n",
        "        data_input_ids.append(sent_input_ids)\n",
        "        data_attention_masks.append(sent_attention_mask)\n",
        "        output_labels.append(label)\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    data_input_ids = torch.stack(data_input_ids, dim=0)\n",
        "    data_attention_masks = torch.stack(data_attention_masks, dim=0)\n",
        "    output_labels = torch.tensor(output_labels)\n",
        "\n",
        "    return data_input_ids, data_attention_masks, output_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0e-1MuX9dcD"
      },
      "source": [
        "def create_loader(tokenizer, sentences, labels, batch_size):\n",
        "\n",
        "    input_ids, attention_masks, labels = tokenize_text(tokenizer, sentences, labels)\n",
        "\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    train_size = len(dataset)\n",
        "\n",
        "    train_dataset = dataset\n",
        "\n",
        "    print('{:>5,} training samples'.format(train_size))\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "    return train_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLJ-hQz19iOa"
      },
      "source": [
        "def train_epoch(train_dataloader, model, optimizer, scheduler):\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_size = b_labels.size(0)\n",
        "\n",
        "        model.zero_grad()\n",
        "        # pdb.set_trace()\n",
        "        outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        smax = torch.softmax(logits, dim = 1)\n",
        "        indices = torch.argmax(smax, dim = 1)\n",
        "        correct += torch.sum(indices==b_labels).item()\n",
        "        total += b_size\n",
        "\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)  \n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "    val_accuracy = correct / total\n",
        "    print(\" Training Accuracy: {0:.4f}\".format(val_accuracy))\n",
        "    print('Time:', time.time() - t0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8xqLymW9q8Z"
      },
      "source": [
        "def eval_epoch(validation_dataloader, model):\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, batch in enumerate(validation_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_size = b_labels.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        smax = torch.softmax(logits, dim = 1)\n",
        "        indices = torch.argmax(smax, dim = 1)\n",
        "\n",
        "        correct += torch.sum(indices==b_labels).item()\n",
        "        total += b_size\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    print(\" Validation Accuracy: {0:.4f}\".format(val_accuracy))\n",
        "    print('Time:', time.time() - t0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPy5tqBFGmfQ"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0yY36yddJR8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}